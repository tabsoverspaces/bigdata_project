{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data - Project\n",
    "\n",
    "## TODO TOPIC\n",
    "\n",
    "### Dragan Postolovsky\n",
    "### Wojciech Taisner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "A 1-page summary of litterature review on studies investigating stackoverflow data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "An enumeration of \"interesting\" investigations directions for the 10-year dataset of stack overflow (from 09/2008 to 09/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What were the questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "A report with analysis results (including visualisation, and statistics) of the 10 years data. These results are the fruit of the investigations projected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:02:57.740285Z",
     "start_time": "2019-01-18T10:02:57.446955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--. 1 wojtek wojtek   346423376 01-18 06:06 AnswersFrame-0.csv.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek    15774436 01-17 11:17 BadgesAggregatedFrame-0.csv.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek   127357784 01-16 12:22 BadgesFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek   204796672 01-09 21:50 Badges.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek     2075408 01-15 21:50 BountyFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek  4093557860 01-10 13:47 Comments.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek     8444828 01-15 19:56 DuplicateFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek    64935436 01-09 20:51 PostLinks.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek   427676924 01-18 02:47 PostsFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek 12303248204 01-10 21:26 Posts.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek      430164 01-15 18:44 TagsFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek      735732 01-09 21:32 Tags.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek   179974748 01-15 21:25 UsersFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek   442348100 01-09 21:54 Users.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek  1002463504 01-09 21:28 Votes.tar.xz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la *.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:01.239644Z",
     "start_time": "2019-01-18T10:03:00.368705Z"
    }
   },
   "outputs": [],
   "source": [
    "# import dask bag\n",
    "import dask.bag as db\n",
    "# import dask datarfame\n",
    "import dask.dataframe as dd\n",
    "# import BeatuifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "# import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:02.889982Z",
     "start_time": "2019-01-18T10:03:02.869124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Badges', 'Comments', 'PostLinks', 'Posts', 'Tags', 'Users', 'Votes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files\n",
    "base = \"stackoverflow.com-\"\n",
    "\n",
    "badges = \"Badges\"\n",
    "comments = \"Comments\"\n",
    "posthistory = \"PostHistory\"\n",
    "postlinks = \"PostLinks\"\n",
    "posts = \"Posts\"\n",
    "tags = \"Tags\"\n",
    "users = \"Users\"\n",
    "votes = \"Votes\"\n",
    "\n",
    "all_files = [badges, comments, postlinks, posts, tags, users, votes] # posthistory, \n",
    "# for some reason post history does not want to co-operate\n",
    "all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "Required in process of transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:05.215628Z",
     "start_time": "2019-01-18T10:03:05.199157Z"
    }
   },
   "outputs": [],
   "source": [
    "# non complex parsing (ex badges, tags, etc)\n",
    "# gets data from single xml row (attr values)\n",
    "# input single line encoded as xml\n",
    "# output tuple of attributes values\n",
    "def xml_row_to_tuple(row): \n",
    "    soup = BeautifulSoup(row, \"xml\")\n",
    "    ret = []\n",
    "    for a, v in soup.row.attrs.items():\n",
    "        if v.isdigit():\n",
    "            v = int(v) #integers take less memory :D\n",
    "        ret.append(v)\n",
    "    return tuple(ret);\n",
    "\n",
    "# gets headers for single xml row (attr names)\n",
    "# input single line encoded as xml\n",
    "# output tuple of attributes names\n",
    "def xml_headers_to_tuple(row): \n",
    "    soup = BeautifulSoup(row, \"xml\")\n",
    "    ret = []\n",
    "    for a, v in soup.row.attrs.items():\n",
    "        ret.append(a)\n",
    "    return tuple(ret);\n",
    "\n",
    "# read data only form provided attrs\n",
    "# input single line encoded as xml d as tuple of required headers\n",
    "# output tuple of values of required headers\n",
    "# special treatment for text values\n",
    "def xml_row_to_tuple_attrs(row, d=tuple(\"Id\")):\n",
    "    soup = BeautifulSoup(row, \"xml\")\n",
    "    ret = []\n",
    "    for e in d:\n",
    "        try:\n",
    "            v = soup.row.attrs[e]\n",
    "        except KeyError:\n",
    "            v = None\n",
    "        \n",
    "        if e == \"Body\":  ## special parser for posts and answers\n",
    "            ret.extend(list(filter_body_noise(v)))\n",
    "        elif e == \"Title\":  ## special parser for posts and answers\n",
    "            ret.append(len(v.split()))\n",
    "        elif v != None:\n",
    "            if v.isdigit():\n",
    "                v=int(v)\n",
    "            ret.append(v)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "    return tuple(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:07.901775Z",
     "start_time": "2019-01-18T10:03:07.881696Z"
    }
   },
   "outputs": [],
   "source": [
    "## select shortest valid header\n",
    "## if any field from tested header is missing in set of headers, then false\n",
    "def is_header_valid(setof, element):\n",
    "    for k in setof:\n",
    "        for e in element:\n",
    "            try:\n",
    "                k.index(e)    \n",
    "            except ValueError:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# looks for first header whicz elements occurs in all the other headers\n",
    "def find_valid_header(headers):\n",
    "    # generate list of sorted headers by length ascending\n",
    "    headers.sort(key=len)\n",
    "    \n",
    "    minlen = min(list(map(len, headers)))\n",
    "    \n",
    "    # for elements where len(element) == min(len(element)) - other just wont be valid anyway\n",
    "    # try to find if it is a valid header\n",
    "    for k in list(filter(lambda x: len(x) == minlen, headers)):\n",
    "        if is_header_valid(headers, k):\n",
    "            return k\n",
    "    return tuple()\n",
    "\n",
    "# returns shortest valid header (it's elements occur in all the other headers)\n",
    "def shortest_valid_header(bag):\n",
    "    # select distinct headers\n",
    "    headers = bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "    return find_valid_header(headers)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:12.793806Z",
     "start_time": "2019-01-18T10:03:12.778253Z"
    }
   },
   "outputs": [],
   "source": [
    "# bag summary header + first row\n",
    "# only bags with proper structure\n",
    "def summary_bag(bag):\n",
    "    th = bag.map(xml_headers_to_tuple).take(1)[0]\n",
    "    for i, v in enumerate(th):\n",
    "        print('{}: {}'.format(i, v))\n",
    "    print(\"\")\n",
    "    tp = bag.map(xml_row_to_tuple).take(1)[0]\n",
    "    for i, v in enumerate(tp):\n",
    "        print('{}: {}'.format(i, v))\n",
    "    print(\"\")\n",
    "    \n",
    "# read bag function\n",
    "def make_xml_bag(filename):\n",
    "    extension=\".tar.xz\"\n",
    "    compression=\"xz\"\n",
    "    temp_bag = db.read_text(filename+extension, compression=compression).str.strip()\n",
    "    temp_bag = temp_bag.filter(lambda x: x.find(\"row\") >= 0)\n",
    "    return temp_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:15.109282Z",
     "start_time": "2019-01-18T10:03:15.098016Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if bag is ok (data are more-a-less structured) [the same number of attributes in each row]\n",
    "# takes a while of course\n",
    "def check_bag(bag):\n",
    "    ma = bag.map(lambda x: len(x)).max().compute()\n",
    "    mi = bag.map(lambda x: len(x)).min().compute()\n",
    "    if ma == mi:\n",
    "        return True\n",
    "    print(ma, mi)\n",
    "    return False\n",
    "\n",
    "# generate metadata iterable of tuples (name, dtype)\n",
    "def meta_from_header(header):\n",
    "    ret = []\n",
    "    for k in header:\n",
    "        if k.find(\"Date\") >= 0:\n",
    "            ret.append((k, np.dtype('datetime64[ns]')))\n",
    "        elif k.find(\"Id\") >= 0 or k.find(\"Score\") >= 0 or k.find(\"Count\") >= 0:\n",
    "            ret.append((k, np.int_))\n",
    "        else:\n",
    "            ret.append((k, np.dtype('S'))) ## TODO some better parsing to string type\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:19.510329Z",
     "start_time": "2019-01-18T10:03:19.495315Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_body_noise(input):\n",
    "\n",
    "    # count blocks of code\n",
    "    nr_code_blocks = len(re.findall(r\"</code>\", input))\n",
    "    # count link in body\n",
    "    nr_links = len(re.findall(r\"</a>\", input))\n",
    "\n",
    "    # couldnt make the regex work with multi-line files, so I remove them :))))\n",
    "    input = input.replace('\\n', ' ')\n",
    "    input = input.replace(\"\\'\", '')\n",
    "\n",
    "    # remove code blocks from body\n",
    "    input = re.sub(r\"<code>(.*?)</code>\", \"\", input,flags=re.MULTILINE)\n",
    "\n",
    "    # remove all other tags from body\n",
    "    input = re.sub(r\"<[^>]*>\", \"\", input)\n",
    "\n",
    "    # remove hardcoded links\n",
    "    input = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', input, flags=re.MULTILINE)\n",
    "\n",
    "    # remove multiple whitespace characters to clean up the previous\n",
    "    # string substitutions(because when removing for example a code block, there will be two spaces\n",
    "    # remaining on each side of the code block, thus we get two whitespace characters\n",
    "    input = re.sub(\"\\s\\s+\", ' ', input)\n",
    "\n",
    "    # count remaining words\n",
    "    nr_words = len(input.split(' '))\n",
    "\n",
    "    # returns tuple in format (number_of_words, number_of_links, number_of_code_blocks)\n",
    "    return (nr_words, nr_links, nr_code_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming input data\n",
    "\n",
    "In this part of project, aim is to clean data, remove xml overhead, mine some of required features and in general minify size of files as possible. Output is tabled-schema data in csv files, with headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:33.813413Z",
     "start_time": "2019-01-18T10:03:33.798452Z"
    }
   },
   "outputs": [],
   "source": [
    "# read tags bag, select rows only\n",
    "tags_bag = db.read_text(tags+\".tar.xz\", compression=\"xz\").str.strip()\n",
    "tags_bag = tags_bag.filter(lambda x: x.find(\"row\") >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:40.916226Z",
     "start_time": "2019-01-18T10:03:40.909804Z"
    }
   },
   "outputs": [],
   "source": [
    "# no need to run this\n",
    "#dt = tags_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:03:45.439724Z",
     "start_time": "2019-01-18T10:03:45.428663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Id', 'TagName', 'Count')\n",
      "CPU times: user 229 µs, sys: 125 µs, total: 354 µs\n",
      "Wall time: 266 µs\n"
     ]
    }
   ],
   "source": [
    "dt = [('Id', 'TagName', 'Count', 'ExcerptPostId', 'WikiPostId'), ('Id', 'TagName', 'Count')]\n",
    "if len(dt) > 1:\n",
    "    header = find_valid_header(dt)\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T10:31:55.762172Z",
     "start_time": "2019-01-18T10:31:43.591910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 34.5 ms, total: 12.1 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "tags_bag2 = tags_bag.map(xml_row_to_tuple_attrs, d=header)\n",
    "tags_df = tags_bag2.to_dataframe(meta=meta_from_header(header))\n",
    "tags_df.to_csv(\"TagsFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Badges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T08:39:12.885891Z",
     "start_time": "2019-01-17T08:39:12.880265Z"
    }
   },
   "outputs": [],
   "source": [
    "badges_bag = make_xml_bag(badges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T08:39:21.303472Z",
     "start_time": "2019-01-17T08:39:21.301292Z"
    }
   },
   "outputs": [],
   "source": [
    "## no need to run this, header provided next cell\n",
    "#dba = badges_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T08:39:28.101096Z",
     "start_time": "2019-01-17T08:39:28.097076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "dba = [('Id', 'UserId', 'Name', 'Date', 'Class', 'TagBased')]\n",
    "if len(dba) > 1:\n",
    "    b_header = find_valid_header(dba)\n",
    "    print(b_header)\n",
    "hhh=('Id', 'UserId', 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T11:22:29.884839Z",
     "start_time": "2019-01-16T09:36:52.789767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 45min 18s, sys: 12.3 s, total: 1h 45min 31s\n",
      "Wall time: 1h 45min 37s\n"
     ]
    }
   ],
   "source": [
    "temp_b = badges_bag.map(xml_row_to_tuple_attrs, d=hhh).to_dataframe(meta=meta_from_header(hhh))\n",
    "temp_b.to_csv(\"BadgesFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T10:17:07.973246Z",
     "start_time": "2019-01-17T08:39:47.213078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BadgesAggregatedFrame-0.csv.tar.xz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate aggregated badges by user id\n",
    "def binop(total,x): #count\n",
    "    return (total[0]+1,)\n",
    "\n",
    "def combine(t1, t2): #combine count\n",
    "    return(t1[0]+t2[0],)\n",
    "\n",
    "hhh2 = (\"UserId\", \"BadgesCount\")\n",
    "temp_b2 = badges_bag.map(xml_row_to_tuple_attrs, d=hhh).foldby(lambda x: x[1], binop, (0,), combine, (0,))\n",
    "temp_b2 = temp_b2.map(lambda x: (x[0], x[1][0])).to_dataframe(meta=meta_from_header(hhh2))\n",
    "temp_b2.to_csv(\"BadgesAggregatedFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_bag = make_xml_bag(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Id', 'PostId', 'Score', 'Text', 'CreationDate', 'UserId'), ('Id', 'PostId', 'Score', 'Text', 'CreationDate', 'UserDisplayName', 'UserId'), ('Id', 'PostId', 'Score', 'Text', 'CreationDate'), ('Id', 'PostId', 'Score', 'Text', 'CreationDate', 'UserDisplayName')]\n",
      "CPU times: user 6.35 s, sys: 2.32 s, total: 8.67 s\n",
      "Wall time: 3h 47min 5s\n"
     ]
    }
   ],
   "source": [
    "# no need to rerun this\n",
    "#dcb = comments_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Id', 'PostId', 'Score', 'Text', 'CreationDate')\n",
      "CPU times: user 241 µs, sys: 0 ns, total: 241 µs\n",
      "Wall time: 190 µs\n"
     ]
    }
   ],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "#if len(dcb) > 1:\n",
    "#    b_header = find_valid_header(dcb)\n",
    "#    print(b_header)\n",
    "    \n",
    "c_header = ('Id', 'PostId', 'Score', 'Text', 'CreationDate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "postlinks_bag = make_xml_bag(postlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Id', 'CreationDate', 'PostId', 'RelatedPostId', 'LinkTypeId')]\n",
      "CPU times: user 765 ms, sys: 275 ms, total: 1.04 s\n",
      "Wall time: 14min 14s\n"
     ]
    }
   ],
   "source": [
    "#no need to rerun this\n",
    "#dplb = postlinks_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dplb)\n",
    "dplb = [('Id', 'CreationDate', 'PostId', 'RelatedPostId', 'LinkTypeId')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "if len(dplb) > 1:\n",
    "    pl_header = find_valid_header(dplb)\n",
    "    print(pl_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 235 ms, total: 3min 34s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "##('Id', 'CreationDate', 'PostId', 'RelatedPostId', 'LinkTypeId')\n",
    "temp_pl = postlinks_bag.filter(lambda x: x.find(\"LinkTypeId=\\\"3\\\"\") >= 0).map(xml_row_to_tuple_attrs, d=dplb[0])\n",
    "temp_pl = temp_pl.to_dataframe(meta=meta_from_header(dplb[0]))\n",
    "temp_pl.to_csv(\"DuplicateFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_bag = make_xml_bag(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T11:19:50.484837Z",
     "start_time": "2019-01-26T11:19:50.478657Z"
    }
   },
   "outputs": [],
   "source": [
    "## do not run this, no point\n",
    "#dub = users_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T11:19:53.817877Z",
     "start_time": "2019-01-26T11:19:53.816048Z"
    }
   },
   "outputs": [],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "#if len(dub) > 1:\n",
    "#    u_header = find_valid_header(dub)\n",
    "#    print(u_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57min 46s, sys: 6.97 s, total: 57min 53s\n",
      "Wall time: 57min 52s\n"
     ]
    }
   ],
   "source": [
    "u_header = ('Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'Views', 'UpVotes', 'DownVotes')\n",
    "temp_u = users_bag.map(xml_row_to_tuple_attrs, d=u_header)\n",
    "temp_u.to_dataframe(meta=meta_from_header(u_header)).to_csv(\"UsersFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_bag = make_xml_bag(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Id', 'PostId', 'VoteTypeId', 'UserId', 'CreationDate'), ('Id', 'PostId', 'VoteTypeId', 'CreationDate', 'BountyAmount'), ('Id', 'PostId', 'VoteTypeId', 'UserId', 'CreationDate', 'BountyAmount'), ('Id', 'PostId', 'VoteTypeId', 'CreationDate')]\n",
      "CPU times: user 15 s, sys: 5.59 s, total: 20.6 s\n",
      "Wall time: 8h 16min 4s\n"
     ]
    }
   ],
   "source": [
    "### dont run this\n",
    "### never ever\n",
    "#CPU times: user 15 s, sys: 5.59 s, total: 20.6 s\n",
    "#Wall time: 8h 16min 4sys\n",
    "## result copied cell below\n",
    "\n",
    "#dv = votes_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Id', 'PostId', 'VoteTypeId', 'CreationDate')\n",
      "CPU times: user 1.83 ms, sys: 0 ns, total: 1.83 ms\n",
      "Wall time: 1.13 ms\n"
     ]
    }
   ],
   "source": [
    "dv = [('Id', 'PostId', 'VoteTypeId', 'UserId', 'CreationDate'), ('Id', 'PostId', 'VoteTypeId', 'CreationDate', 'BountyAmount'), ('Id', 'PostId', 'VoteTypeId', 'UserId', 'CreationDate', 'BountyAmount'), ('Id', 'PostId', 'VoteTypeId', 'CreationDate')]\n",
    "## only one distinct header, do not need to compute this\n",
    "if len(dv) > 1:\n",
    "    v_header = find_valid_header(dv)\n",
    "    print(v_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 49s, sys: 707 ms, total: 7min 50s\n",
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "# only votes to open and close bounty\n",
    "temp_v = votes_bag.filter(lambda x: x.find(\"VoteTypeId=\\\"8\\\"\") >= 0 or x.find(\"VoteTypeId=\\\"9\\\"\") >=0)\n",
    "temp_v = temp_v.map(xml_row_to_tuple_attrs, d=v_header)\n",
    "temp_v.to_dataframe(meta=meta_from_header(v_header)).to_csv(\"BountyFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts History\n",
    "Seems that data are too big and too complicated to process them. Required effort seems to have no profit to give."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts and Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T22:52:22.467340Z",
     "start_time": "2019-01-17T22:52:22.457898Z"
    }
   },
   "outputs": [],
   "source": [
    "# read posts file\n",
    "both_bag = db.read_text(posts+\".tar.xz\", compression=\"xz\").str.strip()\n",
    "both_bag = both_bag.filter(lambda x: x.find('row') >=0 )\n",
    "# create answers and posts bag\n",
    "answers_bag = both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"2\\\"\") >= 0)\n",
    "posts_bag = both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"1\\\"\") >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T22:52:25.030088Z",
     "start_time": "2019-01-17T22:52:25.024811Z"
    }
   },
   "outputs": [],
   "source": [
    "# invastigatimg missing rows from posts\n",
    "#totc = both_bag.count().compute()\n",
    "#totc # total count\n",
    "#ansc = answers_bag.count().compute()\n",
    "#ansc # answers count\n",
    "#posc = posts_bag.count().compute()\n",
    "#posc # posts count\n",
    "#missing = totc - ansc - posc # how many missing\n",
    "#missing\n",
    "# try to find missing vol 2 works\n",
    "#both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"1\\\"\") < 0 and x.find(\"PostTypeId=\\\"2\\\"\") < 0).count().compute()\n",
    "# check whats wrong with data -> undefined types of posts\n",
    "#both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"1\\\"\") < 0 and x.find(\"PostTypeId=\\\"2\\\"\") < 0).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T13:04:03.752154Z",
     "start_time": "2019-01-16T11:23:59.063862Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.8 s, sys: 1.99 s, total: 6.79 s\n",
      "Wall time: 1h 40min 4s\n"
     ]
    }
   ],
   "source": [
    "## check data structure across answers\n",
    "## dont run it, takes a while\n",
    "dab = answers_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dab)\n",
    "if len(dab) > 1:\n",
    "    ans_header = find_valid_header(dab)\n",
    "    print(ans_header)\n",
    "#('Id', 'PostTypeId', 'ParentId', 'CreationDate', 'Score', 'Body', 'LastActivityDate', 'CommentCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T05:06:12.780973Z",
     "start_time": "2019-01-18T01:47:57.386002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AnswersFrame-0.csv.tar.xz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## store answers to csv\n",
    "# required header\n",
    "ans_header = ('Id', 'PostTypeId', 'ParentId', 'CreationDate', 'Score', 'Body', 'LastActivityDate', 'CommentCount')\n",
    "# generating dataframe header\n",
    "temp_ans_meta = meta_from_header(('Id', 'PostTypeId', 'ParentId', 'CreationDate', 'Score', \n",
    "                                  'WordCount', 'LinksCount', 'BlocksCount', 'LastActivityDate', 'CommentCount'))\n",
    "# mapping and getting these heders\n",
    "temp_ans = answers_bag.map(xml_row_to_tuple_attrs, d=ans_header)\n",
    "# create dataframe, store as csv\n",
    "temp_ans.to_dataframe(meta=temp_ans_meta).to_csv(\"AnswersFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T14:24:46.052596Z",
     "start_time": "2019-01-16T13:04:03.759029Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.23 s, sys: 1.53 s, total: 5.76 s\n",
      "Wall time: 1h 20min 42s\n"
     ]
    }
   ],
   "source": [
    "## check data structure across posts\n",
    "## dont run it, takes a while\n",
    "dpb = posts_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dpb## only one distinct header, do not need to compute this\n",
    "if len(dpb) > 1:\n",
    "    pos_header = find_valid_header(dpb)\n",
    "    print(pos_header)\n",
    "#('Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T01:47:57.380961Z",
     "start_time": "2019-01-17T22:52:37.544192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PostsFrame-0.csv.tar.xz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## store posts to csv\n",
    "# required posts header\n",
    "pos_head = ('Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'LastActivityDate', \n",
    "            'Title', 'Tags', 'AnswerCount', 'CommentCount', 'AcceptedAnswerId')\n",
    "# generating dataframe header\n",
    "temp_pos_meta = meta_from_header(('Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', \n",
    "                                  'WordCount', 'LinksCount', 'BlocksCount', 'LastActivityDate', \n",
    "                                  'TitleWordsCount', 'Tags', 'AnswerCount', 'CommentCount', 'AcceptedAnswerId'))\n",
    "\n",
    "# mapping and getting these heders\n",
    "temp_pos = posts_bag.map(xml_row_to_tuple_attrs, d=pos_head)\n",
    "# create dataframe,  store as csv\n",
    "temp_pos.to_dataframe(meta=temp_pos_meta).to_csv(\"PostsFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
