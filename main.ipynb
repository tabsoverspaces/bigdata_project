{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data - Project\n",
    "\n",
    "## Initial data pre-processing\n",
    "\n",
    "### Dragan Postolovski\n",
    "### Wojciech Taisner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of report is dedicated to the data files itself. The aim of the code, provided in this section is to reduce data volume, point only significant data and shape them into proper structure. After the pre-processing each file is exported in CSV format, so it can be easily fit into dataframe. \n",
    "\n",
    "Main objectives are to:\n",
    "* get rid on unnecessary and insignificant data\n",
    "* get rid of blocks of text (replace it with text-related values e.g. word count, etc.)\n",
    "* reduce xml overhead\n",
    "* define require structure of data and export it\n",
    "\n",
    "Two most important technologies:\n",
    "* Dask bag and dataframe - to process data in memory\n",
    "* Beautiful Soup - xml parsing\n",
    "\n",
    "NOTE: dataset is compressed with tar.xz instead of 7z\n",
    "\n",
    "NOTE: commented blocks of code were used to seek for differences in structure between data in single file, this code usually takes long to execute and it's not required to build it, since most of values were hardcoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These computations are very time-consuming because compression does not allow block splitting, so dask can only work with single thread. Also parsing xml is very expensive (in terms of computing power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:30:04.827279Z",
     "start_time": "2019-01-28T11:30:04.711117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--. 1 wojtek wojtek   346423376 01-18 06:06 AnswersFrame-0.csv.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek    15774436 01-17 11:17 BadgesAggregatedFrame-0.csv.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek   127357784 01-16 12:22 BadgesFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek   204796672 01-09 21:50 Badges.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek     2075408 01-15 21:50 BountyFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek  4093557860 01-10 13:47 Comments.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek     8444828 01-15 19:56 DuplicateFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek    64935436 01-09 20:51 PostLinks.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek   486344308 01-28 03:54 PostsFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek 12303248204 01-10 21:26 Posts.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek      430164 01-28 12:29 TagsFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek      735732 01-09 21:32 Tags.tar.xz\r\n",
      "-rw-rw-r--. 1 wojtek wojtek   179974748 01-15 21:25 UsersFrame-0.csv.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek   442348100 01-09 21:54 Users.tar.xz\r\n",
      "-rwxrwxrwx. 1 wojtek wojtek  1002463504 01-09 21:28 Votes.tar.xz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la *.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:30:08.568298Z",
     "start_time": "2019-01-28T11:30:08.113327Z"
    }
   },
   "outputs": [],
   "source": [
    "# import dask bag\n",
    "import dask.bag as db\n",
    "# import dask datarfame\n",
    "import dask.dataframe as dd\n",
    "# import BeatuifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "# import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:30:09.369672Z",
     "start_time": "2019-01-28T11:30:09.344575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Badges', 'Comments', 'PostLinks', 'Posts', 'Tags', 'Users', 'Votes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files\n",
    "base = \"stackoverflow.com-\"\n",
    "\n",
    "badges = \"Badges\"\n",
    "comments = \"Comments\"\n",
    "posthistory = \"PostHistory\"\n",
    "postlinks = \"PostLinks\"\n",
    "posts = \"Posts\"\n",
    "tags = \"Tags\"\n",
    "users = \"Users\"\n",
    "votes = \"Votes\"\n",
    "\n",
    "#all_files = [badges, comments, postlinks, posts, tags, users, votes] # posthistory, \n",
    "# for some reason post history does not want to co-operate\n",
    "#all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "Required in process of transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:30:12.354480Z",
     "start_time": "2019-01-28T11:30:12.347920Z"
    }
   },
   "outputs": [],
   "source": [
    "# non complex parsing (ex badges, tags, etc)\n",
    "# gets data from single xml row (attr values)\n",
    "# input single line encoded as xml\n",
    "# output tuple of attributes values\n",
    "def xml_row_to_tuple(row): \n",
    "    soup = BeautifulSoup(row, \"xml\")\n",
    "    ret = []\n",
    "    for a, v in soup.row.attrs.items():\n",
    "        if v.isdigit():\n",
    "            v = int(v) #integers take less memory :D\n",
    "        ret.append(v)\n",
    "    return tuple(ret);\n",
    "\n",
    "# gets headers for single xml row (attr names)\n",
    "# input single line encoded as xml\n",
    "# output tuple of attributes names\n",
    "def xml_headers_to_tuple(row): \n",
    "    soup = BeautifulSoup(row, \"xml\")\n",
    "    ret = []\n",
    "    for a, v in soup.row.attrs.items():\n",
    "        ret.append(a)\n",
    "    return tuple(ret);\n",
    "\n",
    "# read data only form provided attrs\n",
    "# input single line encoded as xml d as tuple of required headers\n",
    "# output tuple of values of required headers\n",
    "# special treatment for text values\n",
    "def xml_row_to_tuple_attrs(row, d=tuple(\"Id\")):\n",
    "    soup = BeautifulSoup(row, \"xml\")\n",
    "    ret = []\n",
    "    for e in d:\n",
    "        try:\n",
    "            v = soup.row.attrs[e]\n",
    "        except KeyError:\n",
    "            v = None\n",
    "        \n",
    "        if e == \"Body\":  ## special parser for posts and answers\n",
    "            ret.extend(list(filter_body_noise(v)))\n",
    "        elif e == \"Title\":  ## special parser for posts and answers\n",
    "            ret.append(len(v.split()))\n",
    "        elif v != None:\n",
    "            if v.isdigit():\n",
    "                v=int(v)\n",
    "            ret.append(v)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "    return tuple(ret)\n",
    "\n",
    "## select shortest valid header\n",
    "## if any field from tested header is missing in set of headers, then false\n",
    "def is_header_valid(setof, element):\n",
    "    for k in setof:\n",
    "        for e in element:\n",
    "            try:\n",
    "                k.index(e)    \n",
    "            except ValueError:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# looks for first header whicz elements occurs in all the other headers\n",
    "def find_valid_header(headers):\n",
    "    # generate list of sorted headers by length ascending\n",
    "    headers.sort(key=len)\n",
    "    \n",
    "    minlen = min(list(map(len, headers)))\n",
    "    \n",
    "    # for elements where len(element) == min(len(element)) - other just wont be valid anyway\n",
    "    # try to find if it is a valid header\n",
    "    for k in list(filter(lambda x: len(x) == minlen, headers)):\n",
    "        if is_header_valid(headers, k):\n",
    "            return k\n",
    "    return tuple()\n",
    "\n",
    "# returns shortest valid header (it's elements occur in all the other headers)\n",
    "def shortest_valid_header(bag):\n",
    "    # select distinct headers\n",
    "    headers = bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "    return find_valid_header(headers)\n",
    "    \n",
    "\n",
    "# bag summary header + first row\n",
    "# only bags with proper structure\n",
    "def summary_bag(bag):\n",
    "    th = bag.map(xml_headers_to_tuple).take(1)[0]\n",
    "    for i, v in enumerate(th):\n",
    "        print('{}: {}'.format(i, v))\n",
    "    print(\"\")\n",
    "    tp = bag.map(xml_row_to_tuple).take(1)[0]\n",
    "    for i, v in enumerate(tp):\n",
    "        print('{}: {}'.format(i, v))\n",
    "    print(\"\")\n",
    "    \n",
    "# read bag function\n",
    "def make_xml_bag(filename):\n",
    "    extension=\".tar.xz\"\n",
    "    compression=\"xz\"\n",
    "    temp_bag = db.read_text(filename+extension, compression=compression).str.strip()\n",
    "    temp_bag = temp_bag.filter(lambda x: x.find(\"row\") >= 0)\n",
    "    return temp_bag\n",
    "\n",
    "# check if bag is ok (data are more-a-less structured) [the same number of attributes in each row]\n",
    "# takes a while of course\n",
    "def check_bag(bag):\n",
    "    ma = bag.map(lambda x: len(x)).max().compute()\n",
    "    mi = bag.map(lambda x: len(x)).min().compute()\n",
    "    if ma == mi:\n",
    "        return True\n",
    "    print(ma, mi)\n",
    "    return False\n",
    "\n",
    "# generate metadata iterable of tuples (name, dtype)\n",
    "def meta_from_header(header):\n",
    "    ret = []\n",
    "    for k in header:\n",
    "        if k.find(\"Date\") >= 0:\n",
    "            ret.append((k, np.dtype('datetime64[ns]')))\n",
    "        elif k.find(\"Id\") >= 0 or k.find(\"Score\") >= 0 or k.find(\"Count\") >= 0:\n",
    "            ret.append((k, np.int_))\n",
    "        else:\n",
    "            ret.append((k, np.dtype('S'))) ## TODO some better parsing to string type\n",
    "    return ret\n",
    "\n",
    "def filter_body_noise(input):\n",
    "\n",
    "    # count blocks of code\n",
    "    nr_code_blocks = len(re.findall(r\"</code>\", input))\n",
    "    # count link in body\n",
    "    nr_links = len(re.findall(r\"</a>\", input))\n",
    "\n",
    "    # couldnt make the regex work with multi-line files, so I remove them :))))\n",
    "    input = input.replace('\\n', ' ')\n",
    "    input = input.replace(\"\\'\", '')\n",
    "\n",
    "    # remove code blocks from body\n",
    "    input = re.sub(r\"<code>(.*?)</code>\", \"\", input,flags=re.MULTILINE)\n",
    "\n",
    "    # remove all other tags from body\n",
    "    input = re.sub(r\"<[^>]*>\", \"\", input)\n",
    "\n",
    "    # remove hardcoded links\n",
    "    input = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', input, flags=re.MULTILINE)\n",
    "\n",
    "    # remove multiple whitespace characters to clean up the previous\n",
    "    # string substitutions(because when removing for example a code block, there will be two spaces\n",
    "    # remaining on each side of the code block, thus we get two whitespace characters\n",
    "    input = re.sub(\"\\s\\s+\", ' ', input)\n",
    "\n",
    "    # count remaining words\n",
    "    nr_words = len(input.split(' '))\n",
    "\n",
    "    # returns tuple in format (number_of_words, number_of_links, number_of_code_blocks)\n",
    "    return (nr_words, nr_links, nr_code_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming input data\n",
    "\n",
    "In this part of project, aim is to clean data, remove xml overhead, mine some of required features and in general minify size of files as possible. Output is tabled-schema data in csv files, with headers.\n",
    "\n",
    "All data are initially filtered so only rows are parsed. Afterwards provided values are taken from rows and exported to dask dataframe and later to csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:30.161350Z",
     "start_time": "2019-01-28T11:29:30.065366Z"
    }
   },
   "outputs": [],
   "source": [
    "# read tags bag, select rows only\n",
    "tags_bag = db.read_text(tags+\".tar.xz\", compression=\"xz\").str.strip()\n",
    "tags_bag = tags_bag.filter(lambda x: x.find(\"row\") >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:30.308127Z",
     "start_time": "2019-01-28T11:29:30.162916Z"
    }
   },
   "outputs": [],
   "source": [
    "# no need to run this\n",
    "#dt = tags_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:30.381092Z",
     "start_time": "2019-01-28T11:29:30.309687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Id', 'TagName', 'Count')\n"
     ]
    }
   ],
   "source": [
    "dt = [('Id', 'TagName', 'Count', 'ExcerptPostId', 'WikiPostId'), ('Id', 'TagName', 'Count')]\n",
    "if len(dt) > 1:\n",
    "    header = find_valid_header(dt)\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:41.690107Z",
     "start_time": "2019-01-28T11:29:30.382707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TagsFrame-0.csv.tar.xz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_bag2 = tags_bag.map(xml_row_to_tuple_attrs, d=header)\n",
    "tags_df = tags_bag2.to_dataframe(meta=meta_from_header(header))\n",
    "tags_df.to_csv(\"TagsFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Badges\n",
    "\n",
    "Badges are aggregated and counted for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:30:25.153944Z",
     "start_time": "2019-01-28T11:30:25.149252Z"
    }
   },
   "outputs": [],
   "source": [
    "badges_bag = make_xml_bag(badges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:41.782398Z",
     "start_time": "2019-01-28T11:29:41.697682Z"
    }
   },
   "outputs": [],
   "source": [
    "## no need to run this, header provided next cell\n",
    "#dba = badges_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:30:27.096487Z",
     "start_time": "2019-01-28T11:30:27.090957Z"
    }
   },
   "outputs": [],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "dba = [('Id', 'UserId', 'Name', 'Date', 'Class', 'TagBased')]\n",
    "if len(dba) > 1:\n",
    "    b_header = find_valid_header(dba)\n",
    "    print(b_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:30:44.385047Z",
     "start_time": "2019-01-28T11:30:44.379633Z"
    }
   },
   "outputs": [],
   "source": [
    "#hhh=('Id', 'UserId', 'Class')\n",
    "#temp_b = badges_bag.map(xml_row_to_tuple_attrs, d=hhh).to_dataframe(meta=meta_from_header(hhh))\n",
    "#temp_b.to_csv(\"BadgesFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T13:01:51.303646Z",
     "start_time": "2019-01-28T11:30:49.332518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BadgesAggregatedFrame-0.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate aggregated badges by user id\n",
    "def binop(total,x): #count\n",
    "    return (total[0]+1,)\n",
    "\n",
    "def combine(t1, t2): #combine count\n",
    "    return(t1[0]+t2[0],)\n",
    "\n",
    "hhh2 = (\"UserId\", \"BadgesCount\")\n",
    "temp_b2 = badges_bag.map(xml_row_to_tuple_attrs, d=hhh).foldby(lambda x: x[1], binop, (0,), combine, (0,))\n",
    "temp_b2 = temp_b2.map(lambda x: (x[0], x[1][0])).to_dataframe(meta=meta_from_header(hhh2))\n",
    "#temp_b2.to_csv(\"BadgesAggregatedFrame-*.csv.tar.xz\",compression=\"xz\",index=False)\n",
    "temp_b2.to_csv(\"BadgesAggregatedFrame-*.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "This file was not used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.771962Z",
     "start_time": "2019-01-28T11:29:25.193Z"
    }
   },
   "outputs": [],
   "source": [
    "comments_bag = make_xml_bag(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.773582Z",
     "start_time": "2019-01-28T11:29:25.196Z"
    }
   },
   "outputs": [],
   "source": [
    "# no need to rerun this\n",
    "#dcb = comments_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.775161Z",
     "start_time": "2019-01-28T11:29:25.198Z"
    }
   },
   "outputs": [],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "#if len(dcb) > 1:\n",
    "#    b_header = find_valid_header(dcb)\n",
    "#    print(b_header)\n",
    "    \n",
    "c_header = ('Id', 'PostId', 'Score', 'Text', 'CreationDate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Links\n",
    "\n",
    "This data were used to define posts marked as duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.776965Z",
     "start_time": "2019-01-28T11:29:25.200Z"
    }
   },
   "outputs": [],
   "source": [
    "postlinks_bag = make_xml_bag(postlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.777835Z",
     "start_time": "2019-01-28T11:29:25.201Z"
    }
   },
   "outputs": [],
   "source": [
    "#no need to rerun this\n",
    "#dplb = postlinks_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dplb)\n",
    "dplb = [('Id', 'CreationDate', 'PostId', 'RelatedPostId', 'LinkTypeId')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.778722Z",
     "start_time": "2019-01-28T11:29:25.202Z"
    }
   },
   "outputs": [],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "if len(dplb) > 1:\n",
    "    pl_header = find_valid_header(dplb)\n",
    "    print(pl_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.780853Z",
     "start_time": "2019-01-28T11:29:25.204Z"
    }
   },
   "outputs": [],
   "source": [
    "##('Id', 'CreationDate', 'PostId', 'RelatedPostId', 'LinkTypeId')\n",
    "temp_pl = postlinks_bag.filter(lambda x: x.find(\"LinkTypeId=\\\"3\\\"\") >= 0).map(xml_row_to_tuple_attrs, d=dplb[0])\n",
    "temp_pl = temp_pl.to_dataframe(meta=meta_from_header(dplb[0]))\n",
    "temp_pl.to_csv(\"DuplicateFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users\n",
    "\n",
    "This file contains important information about users and their activity on stack overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.782348Z",
     "start_time": "2019-01-28T11:29:25.206Z"
    }
   },
   "outputs": [],
   "source": [
    "users_bag = make_xml_bag(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.783891Z",
     "start_time": "2019-01-28T11:29:25.207Z"
    }
   },
   "outputs": [],
   "source": [
    "## do not run this, no point\n",
    "#dub = users_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.786542Z",
     "start_time": "2019-01-28T11:29:25.208Z"
    }
   },
   "outputs": [],
   "source": [
    "## only one distinct header, do not need to compute this\n",
    "#if len(dub) > 1:\n",
    "#    u_header = find_valid_header(dub)\n",
    "#    print(u_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.790183Z",
     "start_time": "2019-01-28T11:29:25.210Z"
    }
   },
   "outputs": [],
   "source": [
    "u_header = ('Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'Views', 'UpVotes', 'DownVotes')\n",
    "temp_u = users_bag.map(xml_row_to_tuple_attrs, d=u_header)\n",
    "temp_u.to_dataframe(meta=meta_from_header(u_header)).to_csv(\"UsersFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votes\n",
    "\n",
    "We use this section to determine which posts contained bounty for providing accepted answer.\n",
    "\n",
    "NOTE: filtering allowed to significantly reduce time required for this file to be parsed (for entire file its more than 8 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.792294Z",
     "start_time": "2019-01-28T11:29:25.211Z"
    }
   },
   "outputs": [],
   "source": [
    "votes_bag = make_xml_bag(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.794592Z",
     "start_time": "2019-01-28T11:29:25.213Z"
    }
   },
   "outputs": [],
   "source": [
    "### dont run this\n",
    "### never ever\n",
    "#CPU times: user 15 s, sys: 5.59 s, total: 20.6 s\n",
    "#Wall time: 8h 16min 4sys\n",
    "## result copied cell below\n",
    "\n",
    "#dv = votes_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.797147Z",
     "start_time": "2019-01-28T11:29:25.215Z"
    }
   },
   "outputs": [],
   "source": [
    "dv = [('Id', 'PostId', 'VoteTypeId', 'UserId', 'CreationDate'), ('Id', 'PostId', 'VoteTypeId', 'CreationDate', 'BountyAmount'), ('Id', 'PostId', 'VoteTypeId', 'UserId', 'CreationDate', 'BountyAmount'), ('Id', 'PostId', 'VoteTypeId', 'CreationDate')]\n",
    "## only one distinct header, do not need to compute this\n",
    "if len(dv) > 1:\n",
    "    v_header = find_valid_header(dv)\n",
    "    print(v_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.799057Z",
     "start_time": "2019-01-28T11:29:25.216Z"
    }
   },
   "outputs": [],
   "source": [
    "# only votes to open and close bounty\n",
    "temp_v = votes_bag.filter(lambda x: x.find(\"VoteTypeId=\\\"8\\\"\") >= 0 or x.find(\"VoteTypeId=\\\"9\\\"\") >=0)\n",
    "temp_v = temp_v.map(xml_row_to_tuple_attrs, d=v_header)\n",
    "#temp_v.to_dataframe(meta=meta_from_header(v_header)).to_csv(\"BountyFrame-*.csv.tar.xz\",compression=\"xz\",index=False)\n",
    "#export without compression\n",
    "temp_v.to_dataframe(meta=meta_from_header(v_header)).to_csv(\"BountyFrame-*.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts History\n",
    "Seems that data are too big and too complicated to process them. Required effort seems to have no profit to give."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts and Answers \n",
    "\n",
    "Single file containing posts and answers was split into two separate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.800192Z",
     "start_time": "2019-01-28T11:29:25.217Z"
    }
   },
   "outputs": [],
   "source": [
    "# read posts file\n",
    "both_bag = db.read_text(posts+\".tar.xz\", compression=\"xz\").str.strip()\n",
    "both_bag = both_bag.filter(lambda x: x.find('row') >=0 )\n",
    "# create answers and posts bag\n",
    "answers_bag = both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"2\\\"\") >= 0)\n",
    "posts_bag = both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"1\\\"\") >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.801240Z",
     "start_time": "2019-01-28T11:29:25.221Z"
    }
   },
   "outputs": [],
   "source": [
    "# invastigatimg missing rows from posts\n",
    "#totc = both_bag.count().compute()\n",
    "#totc # total count\n",
    "#ansc = answers_bag.count().compute()\n",
    "#ansc # answers count\n",
    "#posc = posts_bag.count().compute()\n",
    "#posc # posts count\n",
    "#missing = totc - ansc - posc # how many missing\n",
    "#missing\n",
    "# try to find missing vol 2 works\n",
    "#both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"1\\\"\") < 0 and x.find(\"PostTypeId=\\\"2\\\"\") < 0).count().compute()\n",
    "# check whats wrong with data -> undefined types of posts\n",
    "#both_bag.filter(lambda x: x.find(\"PostTypeId=\\\"1\\\"\") < 0 and x.find(\"PostTypeId=\\\"2\\\"\") < 0).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.801947Z",
     "start_time": "2019-01-28T11:29:25.223Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## check data structure across answers\n",
    "## dont run it, takes a while\n",
    "dab = answers_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dab)\n",
    "if len(dab) > 1:\n",
    "    ans_header = find_valid_header(dab)\n",
    "    print(ans_header)\n",
    "#('Id', 'PostTypeId', 'ParentId', 'CreationDate', 'Score', 'Body', 'LastActivityDate', 'CommentCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.804877Z",
     "start_time": "2019-01-28T11:29:25.230Z"
    }
   },
   "outputs": [],
   "source": [
    "## store answers to csv\n",
    "# required header\n",
    "ans_header = ('Id', 'PostTypeId', 'ParentId', 'CreationDate', 'Score', 'Body', 'LastActivityDate', 'CommentCount')\n",
    "# generating dataframe header\n",
    "temp_ans_meta = meta_from_header(('Id', 'PostTypeId', 'ParentId', 'CreationDate', 'Score', \n",
    "                                  'WordCount', 'LinksCount', 'BlocksCount', 'LastActivityDate', 'CommentCount'))\n",
    "# mapping and getting these heders\n",
    "temp_ans = answers_bag.map(xml_row_to_tuple_attrs, d=ans_header)\n",
    "# create dataframe, store as csv\n",
    "temp_ans.to_dataframe(meta=temp_ans_meta).to_csv(\"AnswersFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.805511Z",
     "start_time": "2019-01-28T11:29:25.233Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## check data structure across posts\n",
    "## dont run it, takes a while\n",
    "dpb = posts_bag.map(xml_headers_to_tuple).distinct().compute()\n",
    "#print(dpb## only one distinct header, do not need to compute this\n",
    "if len(dpb) > 1:\n",
    "    pos_header = find_valid_header(dpb)\n",
    "    print(pos_header)\n",
    "#('Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:29:45.806171Z",
     "start_time": "2019-01-28T11:29:25.235Z"
    }
   },
   "outputs": [],
   "source": [
    "## store posts to csv\n",
    "# required posts header\n",
    "pos_head = ('Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'LastActivityDate', \n",
    "            'Title', 'Tags', 'AnswerCount', 'CommentCount', 'OwnerUserId', 'AcceptedAnswerId')\n",
    "# generating dataframe header\n",
    "temp_pos_meta = meta_from_header(('Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', \n",
    "                                  'WordCount', 'LinksCount', 'BlocksCount', 'LastActivityDate', \n",
    "                                  'TitleWordsCount', 'Tags', 'AnswerCount', \n",
    "                                  'CommentCount', 'OwnerUserId', 'AcceptedAnswerId'))\n",
    "\n",
    "# mapping and getting these heders\n",
    "temp_pos = posts_bag.map(xml_row_to_tuple_attrs, d=pos_head)\n",
    "# create dataframe,  store as csv\n",
    "temp_pos.to_dataframe(meta=temp_pos_meta).to_csv(\"PostsFrame-*.csv.tar.xz\",compression=\"xz\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
